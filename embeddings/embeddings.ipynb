{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e48daa",
   "metadata": {},
   "source": [
    "# Why is embedding necessary\n",
    "The vectors from the basics (i.e. one hot encoding, bag of words, term frequency-inverse document frequency) suffer from problems like sparsity, high dimensionality etc. Hence the scattered vectors obtained cannot be effeciently used for mathematical operations or downstream tasks. \n",
    "\n",
    "> greater the number of words, greater the vector size\n",
    "\n",
    "therefore, we want dense vector (with real numbers) that are small in size and whose size does not increase as the number of words in vocabulary increases. \n",
    "\n",
    "These dense vectors can be used for different operations because they better represent the concept of similarity in between words (based on distribution hypothesis). These dense vectors are called \"word embeddings\"\n",
    "\n",
    "### Distribution Hypothesis\n",
    "Words with similar meanings frequently appear close together i.e words that appear often in the same context have same meaning.\n",
    "\n",
    "This concept was introduced by Mikolov with the framework called Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942462d4",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "The intuition is to predict a word w from it's context. \n",
    "\n",
    "To do this we need Neural Network and a large corpus (collection of large texts that are used for linguistics and research proposes).\n",
    "\n",
    "The neural network is trained to predict which words \"*c*\" are needed near the target word \"*w*\", the weight of the neural network will be the embedding vector. \n",
    "\n",
    "This is a self supervised model and the labels are implicit and we do not describe them.\n",
    "\n",
    "This task is very fast because, \n",
    "\n",
    "- Word2Vec simplifies the idea by turning the task into binary classification. \n",
    "- and, by using logistic regression classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1059f6",
   "metadata": {},
   "source": [
    "### Choices affecting the quality of embedding\n",
    "\n",
    "- **Data quality is critical**:\n",
    "\n",
    "a. Leveraging wikipedia allows better embedding for semantic tasks.\n",
    "\n",
    "b. using news improves performance for synthetic task (a mixture of both a and b is better)\n",
    "pip install gensim\n",
    "Generally a context of 4 is used, but a context window of 2 allows for vectors that better identify parts of speech. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_agents_with_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
